
1. Import necessary libraries:
   - Import the required PySpark libraries and functions, including `SparkSession`, various functions, data types, and `Window` for window operations.

2. Create a Spark session:
   - Initialize a Spark session with specified configurations, such as the application name and master node.

3. Define Kafka parameters:
   - Set up Kafka parameters, including bootstrap servers, topic to subscribe to, and starting offsets.

4. Define a function to calculate the amount:
   - Create a Python function (`calculate_amount`) to calculate the final amount based on specific conditions.

5. Register a User-Defined Function (UDF):
   - Register the `calculate_amount` function as a UDF with a specified data type.

6. Define a function to process Kafka messages:
   - Create a Python function (`process_message`) to structure the incoming Kafka messages.

7. Register the `process_message` function as a UDF.
   - Register the `process_message` function as a UDF to apply it to the Kafka messages.

8. Define a schema for structured data:
   - Create a schema (`sch`) that defines the structure of the data to be extracted from Kafka messages.

9. Read metadata from MySQL tables:
   - Read data from two MySQL tables (`advertiser` and `adslot`) using JDBC connections and store the results in DataFrames (`advertiser_df` and `adslot_df`).

10. Read streaming data from Kafka:
    - Create a streaming DataFrame (`kafka_messages`) that reads data from a Kafka topic.

11. Apply the `process_message` function:
    - Apply the UDF `process_message_udf` to structure the incoming Kafka messages.

12. Join message data with advertiser and adslot data:
    - Join the structured data with data from the `advertiser_df` and `adslot_df` DataFrames based on specific keys.

13. Window-based data manipulation:
    - Create a window operation using the `Window` function to remove duplicate values based on `uniqId` and the hour of `date_time`.

14. Apply specified conditions:
    - Calculate the final amount based on conditions and apply it to the DataFrame.

15. Drop unnecessary columns:
    - Select specific columns from the DataFrame, excluding unnecessary columns.

16. Start a streaming query:
    - Write the processed data as a stream to a CSV file with specified output options and start the streaming query (`final_query`).

17. Await the termination of the streaming query:
    - Use `awaitTermination()` to ensure that the application continues running and processes streaming data until manually terminated.
